{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Introduction to Artificial Neural Networks\n",
    "\n",
    "- 인공신경망(Artificial Neural Networks, ANN)에 영감을 주었던 핵심 아이디어는 인간 뇌의 구조 이다.\n",
    "- ANN은 딥러닝의 핵심이다. 다목적이며, 강력하고 확장성이 뛰어나기 때문에 수십억 개의 이미지분류, 음성인식 서비스, 최고의 비디오 추천 등 크고 복잡한 기계 학습에 이상적이다.\n",
    "- 또한, 수백만 가지의 과거 게임을 검토 한 다음 게임을 통해 세계 챔피언을 이길 수 있었다(알파고)\n",
    "- chapter 10 에서는 최초의 ANN 구조를 간략히 살펴보고 소개한다. 그리고, MLP(Multi-Layer Perception, 다중레이어지각)를 제시하고, 텐서플로우를 사용하여 MNIST 숫자 분류를 해결한다.[3장에서는 scikit-learn으로 소개하였음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From Biological to Artificial Neurons\n",
    "\n",
    "- ANN은 신경생리학자 Warren McCulloch과 수학자 walter pitts에 의해 1943년에 처음 소개되었다.\n",
    "- \"A Logical Calculus of Ideas Immanent in Nervous Activity\", \"신경작용에 내재된 인지의 논리적 계산\" 을 통해 명제논리학을 사용하여 생물학적 뉴런이 동물 두뇌에서 함께 작동하여 복잡한 계산을 수행하는 방법에 대한 간단한 계산 모델을 제시하였다. -> 최초의 ANN 구조\n",
    "- 1980년대 초, 새로운 네트워크 구조가 발명되고 더 나은 학습 기법들이 개발됨에 따라 ANN 에 대한 관심이 되살아났다.\n",
    "- 1990년대에는 support vector machine(5장)과 같은 강력한 다른 ML 기술이 더 나은 결과와 보다 강력한 이론적 토대를 제공한 것 처럼 보였으므로 대부분의 연구자가 선호하게 되었다.\n",
    "\n",
    "\n",
    "  - **[ANN이 우리 삶에 더 깊은 영향을 줄 것이라 예상되는 이유]**\n",
    "     - 현재 신경 네트워크를 학습하는데 사용할 수 있는 방대한 양의 데이터가 있으며, ANN은 매우 크고 복잡한 문제에 대해 다른 ML 기법보다 대개 성능이 우수하다.\n",
    "     - 1990년대 이래로 컴퓨팅 능력이 엄청나게 증가한 덕분에 합리적인 시간에 대규모 신경 네트워크를 학습 할 수 있다.\n",
    "     - 학습 알고리즘이 개선되었다.\n",
    "     - ANN의 이론적 한계 중 일부는 실제로 양성으로 판명되었다. 예를 들어 많은 사람들은  ANN 학습 알고리즘이 지역 최적(Local optima)상태에 머물러 있기 때문에 한계에 도달했다고 생각하지만, 실제로는 거의 드문것으로 나타났다.\n",
    "     - ANN 기반 제품은 정기적으로 헤드라인 뉴스를 통해 관심을 받으며 점점 더 발전하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Biological Neurons\n",
    "- 인공 뉴런에 대해 논의하기 전, 생물학적 뉴런에 대해 간단히 살펴본다.\n",
    "\n",
    "<img src='10-1.png'>\n",
    "\n",
    "- 주로 동물의 대뇌피질에서 발견되는 unusual 세포로 세포핵과 세포의 복잡한 구성요소의 대부분을 포함하는 세포체로 구성되어 있다. 수상돌기라고 불리는 많은 branching extension과 축색돌기(axon)를 포함한다.\n",
    "- 축색돌기의 길이는 세포체의 몇 배나 길거나 수십만 배까지 길어질 수 있다. 말단 근처에서 축색돌기는 축삭끝가지(telodendria)라고 불리는 많은 가지로 쪼개지며, 이 지점의 끝에는 다른 뉴런의 수상돌기(또는 세포체에 직접 연결된)와 연결된 synaptic terminals(또는 간단히 synapse) 라 불리는 극소(minuscule)의 구조물이 있다. \n",
    "- 생물학적 뉴런은 이 시냅스를 통해 다른 뉴런의 신호(signal)라고 불리는 짧은 전기적 자극(electrical impulses)을 받는다. 뉴런이 수 밀리 초 이내에 다른 뉴런으로부터 충분한 수의 신호를 수신하면 자신의 신호를 작동시킨다.\n",
    "-  따라서 개개의 생물학적 뉴런은 오히려 간단한 방식으로 행동하는 것처럼 보이지만,실제로는 수십억 개의 뉴런으로 구성된 광대한 네트워크로 구성되어 있으며, 각 뉴런은 일반적으로 수천 개의 다른 뉴런과 연결되어 있다.\n",
    "- 상당히 복잡한 계산은 매우 단순한 뉴런의 광대한 네트워크에 의해 수행 될 수 있다. 생물학적 신경망 (BNN)의 아키텍처는 여전히 활발한 연구대상이지만, 뇌의 일부분이 매핑되어 있으며 아래 그림에서 볼 수 있듯이 뉴런은 연속적인 계층으로 구성되는 것 처럼 보인다.\n",
    "\n",
    "\n",
    "<img src='10-2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.2 Logical Computations with Neurons\n",
    "\n",
    "- Warren McCulloch과 Walter pitts는 매우 단순한 생물학적 뉴런 모델을 제안 했는데, 나중에 인공뉴런으로 알려지게 되었다.\n",
    "  - 하나 이상의 이진(on/off)입력과 하나의 이진 출력을 가지는 것. 인공 뉴런은 특정 수 이상의 입력이 활성화 되었을때 출력을 활성화 한다.\n",
    "  - 단순화 된 모델을 사용해도 원하는 모든 논리적 명제를 계산하는 인공뉴런 네트원크를 구축하는 것이 가능하다는 것을 보여주었다.\n",
    "  (예) 두 개 이상의 입력이 활성화 되었을 때 뉴런이 활성화 되었다고 가정하고 다양한 논리 연산을 수행하는 몇 개의 ANN.\n",
    "  \n",
    "  \n",
    "  <img src='10-3.png'>\n",
    "  \n",
    "  - 첫 번째 네트워크는 간단한 identity function이다. 뉴런 A가 활성화 되면 뉴런 C도 활성화 된다. (뉴런 A로부터 두 개의 입력 신호를 받기 때문에) 하지만 뉴런 A가 꺼져있으면, 뉴런 C 역시 꺼져있다.(off)\n",
    "  - 두 번째 네트워크는 논리적 AND를 수행한다. 뉴런 C는 뉴런 A와 B가 활성화 된 경우에만 활성화 된다. (단일 입력 신호로는 뉴런 C를 활성화 시키기 충분하지 않다)\n",
    "  - 세 번째 네트워크는 논리적 OR을 수행한다. 뉴런 C는 뉴런A 또는 뉴런B가 활 성화 된 경우 활성화 된다. (둘 다 활성화 되었을 때 포함)\n",
    "  - 마지막으로, 입력 연결이 뉴런의 활동을 억제 할 수 있다고 가정하면, 네 번째 네트워크는 약간 더 복잡한 논리 명제를 계산한다. 만약 뉴런 A가 활성화되고 뉴런 B가 꺼져있으면 뉴런 C가 활성화된다. 만약 뉴런 A가 항상 활성화되어 있으면 논리적 NOT을 얻는다. 즉, 뉴런 B가 꺼져있을 때 뉴런 C가 활성화되어 있고 반대의 경우도 마찬가지이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.3 The Perceptron\n",
    "-  Perceptron은 Frank Rosenblatt가 1957년에 발명한 가장 단순한 ANN 아키텍처 중 하나이다. \n",
    "- 이것은 선형 임계값 단위(LTU; Linear Threshold Unit)라고 부르는 약간 다른 인공 뉴런을 기반으로 한다. \n",
    "\n",
    "<img src='10-4.png'>\n",
    "\n",
    "- 입력 및 출력은 이제 숫자가 되고(on/off 대신) 각 입력의 연결(input connection)은가중치와 연결된다.\n",
    "- LTU는 입력의 가중 합을 계산 한 다음, \n",
    "<img src='10-5.png'>\n",
    "- 그 합에 step function을 적용하여 결과를 출력\n",
    "<img src='10-6.png'>\n",
    "- 퍼셉트론에서 사용되는 가장 일반적인 step function은 Heaviside 함수이다. 때로는 sign 함수가 대신 사용되기도 한다.\n",
    "<img src='10-7.png'>\n",
    "\n",
    "\n",
    "- 퍼셉트론은 LTU의 단일 층으로 구성되며, 각 입력에는 모든 뉴런이 연결된다. 이러한 연결은 input neuron이라고 불리는 특별한 passthrough neurons이라고 표현된다. 이는 모든 입력값을 출력한다. 또한, 추가 bias feature가 일반적으로 추가된다(x0=1). \n",
    "- 이 bias feature는 일반적으로 항상 1을 출력하는 bias neuron이라는 특별한 유형의 뉴런을 사용하여 표현된다. \n",
    "\n",
    "<img src='10-8.png'>\n",
    "\n",
    "- 위 그림은 두 개의 입력과 세개의 출력을 갖는 퍼셉트론을 나타낸다. 이 퍼셉트론은 인스턴스를 동시에 세 가지 다른 이진 클래스로 분류할 수 있으므로 다중 출력 분류기 이다.\n",
    "\n",
    "**[퍼센트론의 학습 방법?]**\n",
    "- Frank Rosenblatt가 제안한 퍼셉트론 학습 알고리즘은 Hebb's rule에 크게 영향을 받았다. \n",
    "- 즉, 동일한 출력을 가질 때마다 두 뉴런 사이의 연결 가중치가 증가한다. 보다 구체적으로 말하면, 퍼셉트론은 한 번에 하나의 학습 인스턴스로 공급되며, 각 인스턴스에 대해 예측을 한다. 잘못된 예측을 산출한 모든 출력 뉴런에 대해 올바른 예측에 기여한 입력으로부터 연결 가중치를 강화한다. rule 식은 아래와 같다.\n",
    "\n",
    "\n",
    "<img src='10-9.png'>\n",
    "\n",
    "- \n",
    "  - w_i,j는 i번째 input neuron과 j번째 output neuron 사이의 연결 가중치이다.\n",
    "  - x_i는 현재 학습 인스턴스의 i번째 입력 값이다.\n",
    "  - y_hat_j는 현재 학습 인스턴스를 위한, j번째 output neuron의 output이다.      \n",
    "  \n",
    "  **???? y_hat_j과,  y_j의 차이?**\n",
    "  \n",
    "  - y_j는 현재 학습 인스턴스를 위한 output neuron의 j번째 target output이다.\n",
    "  - eta는 학습률(learning rate)이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Training a DNN Using Plain Tensorflow\n",
    "- 네트워크 아키텍처를 보다 잘 제어하려면, 텐서플로우의 낮은 수준의 파이썬 API를 사용하는 것이 좋다.\n",
    "- API를 사용하기 전과 동일한 모델을 만들고 MNIST 데이터 세트에서 Mini-batch Gradient Descent를 구현한다.\n",
    "\n",
    "  - 첫 번째, 텐서플로우 그래프를 빌드하는 construction Phase(구축단계)\n",
    "  - 두 번째, execution phase(실행단계). 실제로 그래프를 실행하여 모델을 학습."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"ann\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_setsmnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "y_train = mnist.train.labels.astype(\"int\")\n",
    "y_test = mnist.test.labels.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\tmp9g7bmdju\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_is_chief': True, '_keep_checkpoint_max': 5, '_num_ps_replicas': 0, '_master': '', '_num_worker_replicas': 0, '_tf_random_seed': 42, '_session_config': None, '_evaluation_master': '', '_save_summary_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_model_dir': 'C:\\\\Users\\\\Public\\\\Documents\\\\ESTsoft\\\\CreatorTemp\\\\tmp9g7bmdju', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000185093B2710>, '_environment': 'local', '_task_id': 0, '_keep_checkpoint_every_n_hours': 10000}\n",
      "WARNING:tensorflow:From c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\tmp9g7bmdju\\model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 2.36404\n",
      "INFO:tensorflow:global_step/sec: 391.117\n",
      "INFO:tensorflow:step = 101, loss = 0.311432 (0.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 415.559\n",
      "INFO:tensorflow:step = 201, loss = 0.265409 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.401\n",
      "INFO:tensorflow:step = 301, loss = 0.408733 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 433.625\n",
      "INFO:tensorflow:step = 401, loss = 0.244357 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.303\n",
      "INFO:tensorflow:step = 501, loss = 0.238858 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 428.043\n",
      "INFO:tensorflow:step = 601, loss = 0.091827 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 428.044\n",
      "INFO:tensorflow:step = 701, loss = 0.123374 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.52\n",
      "INFO:tensorflow:step = 801, loss = 0.196473 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 445.242\n",
      "INFO:tensorflow:step = 901, loss = 0.0932024 (0.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.263\n",
      "INFO:tensorflow:step = 1001, loss = 0.196834 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 429.889\n",
      "INFO:tensorflow:step = 1101, loss = 0.194408 (0.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.357\n",
      "INFO:tensorflow:step = 1201, loss = 0.152048 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.357\n",
      "INFO:tensorflow:step = 1301, loss = 0.149761 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.301\n",
      "INFO:tensorflow:step = 1401, loss = 0.0647763 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 428.044\n",
      "INFO:tensorflow:step = 1501, loss = 0.0727728 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 433.627\n",
      "INFO:tensorflow:step = 1601, loss = 0.120371 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.358\n",
      "INFO:tensorflow:step = 1701, loss = 0.0426907 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 445.242\n",
      "INFO:tensorflow:step = 1801, loss = 0.148324 (0.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.519\n",
      "INFO:tensorflow:step = 1901, loss = 0.073107 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 420.82\n",
      "INFO:tensorflow:step = 2001, loss = 0.0638611 (0.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 420.82\n",
      "INFO:tensorflow:step = 2101, loss = 0.0233887 (0.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.301\n",
      "INFO:tensorflow:step = 2201, loss = 0.0329285 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.43\n",
      "INFO:tensorflow:step = 2301, loss = 0.0519914 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 433.626\n",
      "INFO:tensorflow:step = 2401, loss = 0.0577268 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.358\n",
      "INFO:tensorflow:step = 2501, loss = 0.084227 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.357\n",
      "INFO:tensorflow:step = 2601, loss = 0.0315869 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 431.75\n",
      "INFO:tensorflow:step = 2701, loss = 0.0121763 (0.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 433.627\n",
      "INFO:tensorflow:step = 2801, loss = 0.0630663 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.52\n",
      "INFO:tensorflow:step = 2901, loss = 0.0913481 (0.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 386.567\n",
      "INFO:tensorflow:step = 3001, loss = 0.0138151 (0.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 395.77\n",
      "INFO:tensorflow:step = 3101, loss = 0.0337465 (0.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 400.538\n",
      "INFO:tensorflow:step = 3201, loss = 0.0132512 (0.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 405.095\n",
      "INFO:tensorflow:step = 3301, loss = 0.0321816 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 420.819\n",
      "INFO:tensorflow:step = 3401, loss = 0.143895 (0.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 445.242\n",
      "INFO:tensorflow:step = 3501, loss = 0.0897496 (0.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 445.241\n",
      "INFO:tensorflow:step = 3601, loss = 0.157295 (0.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.431\n",
      "INFO:tensorflow:step = 3701, loss = 0.03669 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 429.886\n",
      "INFO:tensorflow:step = 3801, loss = 0.0122543 (0.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.012\n",
      "INFO:tensorflow:step = 3901, loss = 0.152421 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.358\n",
      "INFO:tensorflow:step = 4001, loss = 0.107787 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 445.241\n",
      "INFO:tensorflow:step = 4101, loss = 0.0485792 (0.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 420.816\n",
      "INFO:tensorflow:step = 4201, loss = 0.0590532 (0.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.404\n",
      "INFO:tensorflow:step = 4301, loss = 0.158572 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.302\n",
      "INFO:tensorflow:step = 4401, loss = 0.114563 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.52\n",
      "INFO:tensorflow:step = 4501, loss = 0.0187132 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 405.423\n",
      "INFO:tensorflow:step = 4601, loss = 0.0182185 (0.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 419.857\n",
      "INFO:tensorflow:step = 4701, loss = 0.00904818 (0.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 422.593\n",
      "INFO:tensorflow:step = 4801, loss = 0.0186206 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 410.437\n",
      "INFO:tensorflow:step = 4901, loss = 0.0834735 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 431.75\n",
      "INFO:tensorflow:step = 5001, loss = 0.0451745 (0.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 433.626\n",
      "INFO:tensorflow:step = 5101, loss = 0.00802126 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.521\n",
      "INFO:tensorflow:step = 5201, loss = 0.0236921 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 429.889\n",
      "INFO:tensorflow:step = 5301, loss = 0.040991 (0.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.263\n",
      "INFO:tensorflow:step = 5401, loss = 0.0607277 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 422.601\n",
      "INFO:tensorflow:step = 5501, loss = 0.0441553 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.431\n",
      "INFO:tensorflow:step = 5601, loss = 0.0769239 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.431\n",
      "INFO:tensorflow:step = 5701, loss = 0.0198653 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.263\n",
      "INFO:tensorflow:step = 5801, loss = 0.00881232 (0.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.522\n",
      "INFO:tensorflow:step = 5901, loss = 0.110585 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.299\n",
      "INFO:tensorflow:step = 6001, loss = 0.0857567 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.304\n",
      "INFO:tensorflow:step = 6101, loss = 0.0120617 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 447.238\n",
      "INFO:tensorflow:step = 6201, loss = 0.022921 (0.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 445.241\n",
      "INFO:tensorflow:step = 6301, loss = 0.0707142 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.519\n",
      "INFO:tensorflow:step = 6401, loss = 0.0212568 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 433.628\n",
      "INFO:tensorflow:step = 6501, loss = 0.00796753 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 429.889\n",
      "INFO:tensorflow:step = 6601, loss = 0.0281497 (0.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.431\n",
      "INFO:tensorflow:step = 6701, loss = 0.0216029 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 433.626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:step = 6801, loss = 0.0129809 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.431\n",
      "INFO:tensorflow:step = 6901, loss = 0.0126797 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.302\n",
      "INFO:tensorflow:step = 7001, loss = 0.0166421 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.263\n",
      "INFO:tensorflow:step = 7101, loss = 0.00448675 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.357\n",
      "INFO:tensorflow:step = 7201, loss = 0.0508161 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 433.626\n",
      "INFO:tensorflow:step = 7301, loss = 0.00550252 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.358\n",
      "INFO:tensorflow:step = 7401, loss = 0.0154965 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.302\n",
      "INFO:tensorflow:step = 7501, loss = 0.004822 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.429\n",
      "INFO:tensorflow:step = 7601, loss = 0.0131625 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 433.628\n",
      "INFO:tensorflow:step = 7701, loss = 0.00680886 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.43\n",
      "INFO:tensorflow:step = 7801, loss = 0.00376618 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 407.08\n",
      "INFO:tensorflow:step = 7901, loss = 0.0131479 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 415.559\n",
      "INFO:tensorflow:step = 8001, loss = 0.00631671 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 420.819\n",
      "INFO:tensorflow:step = 8101, loss = 0.0245089 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.125\n",
      "INFO:tensorflow:step = 8201, loss = 0.0208505 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.937\n",
      "INFO:tensorflow:step = 8301, loss = 0.04197 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 391.111\n",
      "INFO:tensorflow:step = 8401, loss = 0.0136715 (0.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 397.349\n",
      "INFO:tensorflow:step = 8501, loss = 0.0110502 (0.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 394.208\n",
      "INFO:tensorflow:step = 8601, loss = 0.00604934 (0.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.937\n",
      "INFO:tensorflow:step = 8701, loss = 0.0065747 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 392.64\n",
      "INFO:tensorflow:step = 8801, loss = 0.00738027 (0.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 394.219\n",
      "INFO:tensorflow:step = 8901, loss = 0.00280681 (0.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 391.114\n",
      "INFO:tensorflow:step = 9001, loss = 0.015022 (0.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 391.115\n",
      "INFO:tensorflow:step = 9101, loss = 0.00840854 (0.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 405.423\n",
      "INFO:tensorflow:step = 9201, loss = 0.00372718 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 422.602\n",
      "INFO:tensorflow:step = 9301, loss = 0.0134235 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.401\n",
      "INFO:tensorflow:step = 9401, loss = 0.034022 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.357\n",
      "INFO:tensorflow:step = 9501, loss = 0.00841827 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 420.819\n",
      "INFO:tensorflow:step = 9601, loss = 0.015819 (0.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 419.051\n",
      "INFO:tensorflow:step = 9701, loss = 0.0086599 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.123\n",
      "INFO:tensorflow:step = 9801, loss = 0.00439482 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 419.053\n",
      "INFO:tensorflow:step = 9901, loss = 0.0174038 (0.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 415.559\n",
      "INFO:tensorflow:step = 10001, loss = 0.0208312 (0.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.401\n",
      "INFO:tensorflow:step = 10101, loss = 0.00381713 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.125\n",
      "INFO:tensorflow:step = 10201, loss = 0.00676587 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.431\n",
      "INFO:tensorflow:step = 10301, loss = 0.00574352 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 426.214\n",
      "INFO:tensorflow:step = 10401, loss = 0.00939435 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 431.749\n",
      "INFO:tensorflow:step = 10501, loss = 0.00275532 (0.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 420.82\n",
      "INFO:tensorflow:step = 10601, loss = 0.00952128 (0.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.401\n",
      "INFO:tensorflow:step = 10701, loss = 0.0303758 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 431.75\n",
      "INFO:tensorflow:step = 10801, loss = 0.0115768 (0.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 433.627\n",
      "INFO:tensorflow:step = 10901, loss = 0.00465772 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 422.602\n",
      "INFO:tensorflow:step = 11001, loss = 0.0293584 (0.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.401\n",
      "INFO:tensorflow:step = 11101, loss = 0.00492727 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.52\n",
      "INFO:tensorflow:step = 11201, loss = 0.000853527 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.52\n",
      "INFO:tensorflow:step = 11301, loss = 0.0139262 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 433.626\n",
      "INFO:tensorflow:step = 11401, loss = 0.00881559 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.401\n",
      "INFO:tensorflow:step = 11501, loss = 0.0186049 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 433.627\n",
      "INFO:tensorflow:step = 11601, loss = 0.000492324 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 419.049\n",
      "INFO:tensorflow:step = 11701, loss = 0.00230341 (0.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 428.046\n",
      "INFO:tensorflow:step = 11801, loss = 0.000418936 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 417.297\n",
      "INFO:tensorflow:step = 11901, loss = 0.00709947 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 433.628\n",
      "INFO:tensorflow:step = 12001, loss = 0.000176346 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.52\n",
      "INFO:tensorflow:step = 12101, loss = 0.00352114 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.521\n",
      "INFO:tensorflow:step = 12201, loss = 0.00409825 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 408.746\n",
      "INFO:tensorflow:step = 12301, loss = 0.00442711 (0.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 400.539\n",
      "INFO:tensorflow:step = 12401, loss = 0.000686727 (0.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 373.536\n",
      "INFO:tensorflow:step = 12501, loss = 0.00342111 (0.269 sec)\n",
      "INFO:tensorflow:global_step/sec: 391.114\n",
      "INFO:tensorflow:step = 12601, loss = 0.00143894 (0.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 429.889\n",
      "INFO:tensorflow:step = 12701, loss = 0.00396524 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.937\n",
      "INFO:tensorflow:step = 12801, loss = 0.00715362 (0.249 sec)\n",
      "INFO:tensorflow:global_step/sec: 400.539\n",
      "INFO:tensorflow:step = 12901, loss = 0.00564594 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 380.663\n",
      "INFO:tensorflow:step = 13001, loss = 0.00317152 (0.263 sec)\n",
      "INFO:tensorflow:global_step/sec: 382.125\n",
      "INFO:tensorflow:step = 13101, loss = 0.00411803 (0.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 415.558\n",
      "INFO:tensorflow:step = 13201, loss = 0.00683057 (0.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 426.215\n",
      "INFO:tensorflow:step = 13301, loss = 0.00482892 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.124\n",
      "INFO:tensorflow:step = 13401, loss = 0.00194253 (0.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 408.747\n",
      "INFO:tensorflow:step = 13501, loss = 0.00725991 (0.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 433.627\n",
      "INFO:tensorflow:step = 13601, loss = 0.00496594 (0.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.401\n",
      "INFO:tensorflow:step = 13701, loss = 0.00231069 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 415.559\n",
      "INFO:tensorflow:step = 13801, loss = 0.00758488 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 417.298\n",
      "INFO:tensorflow:step = 13901, loss = 0.00351695 (0.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 402.153\n",
      "INFO:tensorflow:step = 14001, loss = 0.00250569 (0.249 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.125\n",
      "INFO:tensorflow:step = 14101, loss = 0.00576778 (0.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 420.82\n",
      "INFO:tensorflow:step = 14201, loss = 0.0106178 (0.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 397.347\n",
      "INFO:tensorflow:step = 14301, loss = 0.00184563 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 420.819\n",
      "INFO:tensorflow:step = 14401, loss = 0.000657787 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.302\n",
      "INFO:tensorflow:step = 14501, loss = 0.000907568 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.401\n",
      "INFO:tensorflow:step = 14601, loss = 0.00308903 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 403.781\n",
      "INFO:tensorflow:step = 14701, loss = 0.000958802 (0.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 420.82\n",
      "INFO:tensorflow:step = 14801, loss = 0.00119474 (0.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 397.348\n",
      "INFO:tensorflow:step = 14901, loss = 0.00213329 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 397.347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:step = 15001, loss = 0.0011101 (0.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 397.348\n",
      "INFO:tensorflow:step = 15101, loss = 0.00242632 (0.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.935\n",
      "INFO:tensorflow:step = 15201, loss = 0.00159225 (0.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 417.3\n",
      "INFO:tensorflow:step = 15301, loss = 0.0014488 (0.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 394.206\n",
      "INFO:tensorflow:step = 15401, loss = 0.00376972 (0.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 397.347\n",
      "INFO:tensorflow:step = 15501, loss = 0.00406733 (0.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.937\n",
      "INFO:tensorflow:step = 15601, loss = 0.00472252 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.934\n",
      "INFO:tensorflow:step = 15701, loss = 0.0146699 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 395.771\n",
      "INFO:tensorflow:step = 15801, loss = 0.00153426 (0.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 397.348\n",
      "INFO:tensorflow:step = 15901, loss = 0.000447847 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 391.109\n",
      "INFO:tensorflow:step = 16001, loss = 0.00656253 (0.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 405.429\n",
      "INFO:tensorflow:step = 16101, loss = 0.0043496 (0.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 433.626\n",
      "INFO:tensorflow:step = 16201, loss = 7.70298e-05 (0.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 426.202\n",
      "INFO:tensorflow:step = 16301, loss = 0.00260926 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.415\n",
      "INFO:tensorflow:step = 16401, loss = 0.00155471 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.4\n",
      "INFO:tensorflow:step = 16501, loss = 0.0015937 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 417.298\n",
      "INFO:tensorflow:step = 16601, loss = 0.00472044 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 422.603\n",
      "INFO:tensorflow:step = 16701, loss = 0.00249327 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 420.819\n",
      "INFO:tensorflow:step = 16801, loss = 0.00237052 (0.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 419.051\n",
      "INFO:tensorflow:step = 16901, loss = 0.00251921 (0.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 419.051\n",
      "INFO:tensorflow:step = 17001, loss = 0.00285329 (0.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 422.602\n",
      "INFO:tensorflow:step = 17101, loss = 0.000563493 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 403.782\n",
      "INFO:tensorflow:step = 17201, loss = 0.00264452 (0.249 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.937\n",
      "INFO:tensorflow:step = 17301, loss = 0.000915113 (0.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 410.428\n",
      "INFO:tensorflow:step = 17401, loss = 0.00186997 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 426.214\n",
      "INFO:tensorflow:step = 17501, loss = 0.00155757 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.123\n",
      "INFO:tensorflow:step = 17601, loss = 0.000356618 (0.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 413.837\n",
      "INFO:tensorflow:step = 17701, loss = 0.000703532 (0.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 422.602\n",
      "INFO:tensorflow:step = 17801, loss = 0.000502199 (0.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.263\n",
      "INFO:tensorflow:step = 17901, loss = 0.00201421 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 433.626\n",
      "INFO:tensorflow:step = 18001, loss = 0.00172619 (0.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 402.155\n",
      "INFO:tensorflow:step = 18101, loss = 0.000986052 (0.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 428.043\n",
      "INFO:tensorflow:step = 18201, loss = 0.00447541 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.401\n",
      "INFO:tensorflow:step = 18301, loss = 0.0146146 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 415.559\n",
      "INFO:tensorflow:step = 18401, loss = 0.00289675 (0.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 408.747\n",
      "INFO:tensorflow:step = 18501, loss = 0.0022742 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.263\n",
      "INFO:tensorflow:step = 18601, loss = 0.00423942 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.357\n",
      "INFO:tensorflow:step = 18701, loss = 0.000945454 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 422.602\n",
      "INFO:tensorflow:step = 18801, loss = 0.00149365 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.521\n",
      "INFO:tensorflow:step = 18901, loss = 0.00309862 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 413.834\n",
      "INFO:tensorflow:step = 19001, loss = 0.00134573 (0.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 403.783\n",
      "INFO:tensorflow:step = 19101, loss = 0.000693691 (0.249 sec)\n",
      "INFO:tensorflow:global_step/sec: 400.539\n",
      "INFO:tensorflow:step = 19201, loss = 0.000465131 (0.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 403.782\n",
      "INFO:tensorflow:step = 19301, loss = 0.00558755 (0.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 369.385\n",
      "INFO:tensorflow:step = 19401, loss = 0.000528183 (0.271 sec)\n",
      "INFO:tensorflow:global_step/sec: 389.587\n",
      "INFO:tensorflow:step = 19501, loss = 0.00208042 (0.257 sec)\n",
      "INFO:tensorflow:global_step/sec: 417.298\n",
      "INFO:tensorflow:step = 19601, loss = 0.000694155 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 415.559\n",
      "INFO:tensorflow:step = 19701, loss = 0.000119371 (0.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 413.835\n",
      "INFO:tensorflow:step = 19801, loss = 0.00051242 (0.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 445.242\n",
      "INFO:tensorflow:step = 19901, loss = 0.00184305 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.335\n",
      "INFO:tensorflow:step = 20001, loss = 0.00206445 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 447.261\n",
      "INFO:tensorflow:step = 20101, loss = 0.0003063 (0.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 445.242\n",
      "INFO:tensorflow:step = 20201, loss = 0.00195141 (0.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.519\n",
      "INFO:tensorflow:step = 20301, loss = 0.000959544 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 417.298\n",
      "INFO:tensorflow:step = 20401, loss = 0.00013434 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 428.044\n",
      "INFO:tensorflow:step = 20501, loss = 0.0003517 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 445.241\n",
      "INFO:tensorflow:step = 20601, loss = 0.00149112 (0.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 402.153\n",
      "INFO:tensorflow:step = 20701, loss = 0.00112818 (0.249 sec)\n",
      "INFO:tensorflow:global_step/sec: 372.143\n",
      "INFO:tensorflow:step = 20801, loss = 0.00154887 (0.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 394.207\n",
      "INFO:tensorflow:step = 20901, loss = 0.00139391 (0.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 415.541\n",
      "INFO:tensorflow:step = 21001, loss = 0.00281828 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.451\n",
      "INFO:tensorflow:step = 21101, loss = 0.00136329 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.262\n",
      "INFO:tensorflow:step = 21201, loss = 0.00240587 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.359\n",
      "INFO:tensorflow:step = 21301, loss = 0.00113844 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.43\n",
      "INFO:tensorflow:step = 21401, loss = 0.00262636 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 447.239\n",
      "INFO:tensorflow:step = 21501, loss = 0.000237559 (0.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.356\n",
      "INFO:tensorflow:step = 21601, loss = 0.00151222 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 426.215\n",
      "INFO:tensorflow:step = 21701, loss = 0.000995086 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 428.044\n",
      "INFO:tensorflow:step = 21801, loss = 0.000349444 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.357\n",
      "INFO:tensorflow:step = 21901, loss = 0.000656097 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.521\n",
      "INFO:tensorflow:step = 22001, loss = 0.000148364 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 395.77\n",
      "INFO:tensorflow:step = 22101, loss = 0.000531831 (0.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 386.567\n",
      "INFO:tensorflow:step = 22201, loss = 0.00140872 (0.258 sec)\n",
      "INFO:tensorflow:global_step/sec: 389.586\n",
      "INFO:tensorflow:step = 22301, loss = 0.00243447 (0.257 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.937\n",
      "INFO:tensorflow:step = 22401, loss = 0.00220339 (0.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 403.782\n",
      "INFO:tensorflow:step = 22501, loss = 0.00157356 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 407.078\n",
      "INFO:tensorflow:step = 22601, loss = 0.00364198 (0.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 400.538\n",
      "INFO:tensorflow:step = 22701, loss = 0.000852597 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 405.424\n",
      "INFO:tensorflow:step = 22801, loss = 0.00109362 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 397.347\n",
      "INFO:tensorflow:step = 22901, loss = 0.00240539 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 397.347\n",
      "INFO:tensorflow:step = 23001, loss = 0.000654596 (0.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 392.654\n",
      "INFO:tensorflow:step = 23101, loss = 0.00295467 (0.255 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 407.078\n",
      "INFO:tensorflow:step = 23201, loss = 0.00217089 (0.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.519\n",
      "INFO:tensorflow:step = 23301, loss = 0.00169613 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.265\n",
      "INFO:tensorflow:step = 23401, loss = 0.000720154 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.302\n",
      "INFO:tensorflow:step = 23501, loss = 0.000805685 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 410.429\n",
      "INFO:tensorflow:step = 23601, loss = 0.000631053 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 407.077\n",
      "INFO:tensorflow:step = 23701, loss = 0.000255326 (0.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 431.751\n",
      "INFO:tensorflow:step = 23801, loss = 0.00180432 (0.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 389.585\n",
      "INFO:tensorflow:step = 23901, loss = 0.000976085 (0.257 sec)\n",
      "INFO:tensorflow:global_step/sec: 386.568\n",
      "INFO:tensorflow:step = 24001, loss = 0.00113664 (0.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 392.654\n",
      "INFO:tensorflow:step = 24101, loss = 0.000741892 (0.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 392.655\n",
      "INFO:tensorflow:step = 24201, loss = 0.000928554 (0.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 407.078\n",
      "INFO:tensorflow:step = 24301, loss = 0.000218442 (0.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 407.078\n",
      "INFO:tensorflow:step = 24401, loss = 0.00173174 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 389.587\n",
      "INFO:tensorflow:step = 24501, loss = 0.000569286 (0.257 sec)\n",
      "INFO:tensorflow:global_step/sec: 408.745\n",
      "INFO:tensorflow:step = 24601, loss = 0.000322807 (0.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 408.747\n",
      "INFO:tensorflow:step = 24701, loss = 0.00111121 (0.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 394.207\n",
      "INFO:tensorflow:step = 24801, loss = 0.00132868 (0.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 394.205\n",
      "INFO:tensorflow:step = 24901, loss = 0.00235335 (0.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 391.113\n",
      "INFO:tensorflow:step = 25001, loss = 0.000118122 (0.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.126\n",
      "INFO:tensorflow:step = 25101, loss = 0.00184492 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 417.298\n",
      "INFO:tensorflow:step = 25201, loss = 0.00320799 (0.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 410.428\n",
      "INFO:tensorflow:step = 25301, loss = 0.000183373 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 373.536\n",
      "INFO:tensorflow:step = 25401, loss = 0.000734714 (0.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 394.207\n",
      "INFO:tensorflow:step = 25501, loss = 0.00117328 (0.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 402.154\n",
      "INFO:tensorflow:step = 25601, loss = 0.00101388 (0.249 sec)\n",
      "INFO:tensorflow:global_step/sec: 392.654\n",
      "INFO:tensorflow:step = 25701, loss = 0.000994076 (0.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.401\n",
      "INFO:tensorflow:step = 25801, loss = 0.00148109 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.302\n",
      "INFO:tensorflow:step = 25901, loss = 0.00150336 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.302\n",
      "INFO:tensorflow:step = 26001, loss = 4.42679e-05 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 428.043\n",
      "INFO:tensorflow:step = 26101, loss = 0.000945769 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.937\n",
      "INFO:tensorflow:step = 26201, loss = 0.000692925 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 429.885\n",
      "INFO:tensorflow:step = 26301, loss = 0.000701611 (0.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.361\n",
      "INFO:tensorflow:step = 26401, loss = 0.00164209 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 445.241\n",
      "INFO:tensorflow:step = 26501, loss = 0.000905918 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.263\n",
      "INFO:tensorflow:step = 26601, loss = 0.000519783 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.357\n",
      "INFO:tensorflow:step = 26701, loss = 1.40954e-05 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.302\n",
      "INFO:tensorflow:step = 26801, loss = 0.000842135 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 451.286\n",
      "INFO:tensorflow:step = 26901, loss = 0.000808341 (0.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.263\n",
      "INFO:tensorflow:step = 27001, loss = 0.000642265 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 431.749\n",
      "INFO:tensorflow:step = 27101, loss = 0.000423766 (0.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.432\n",
      "INFO:tensorflow:step = 27201, loss = 0.000671817 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.43\n",
      "INFO:tensorflow:step = 27301, loss = 0.00108788 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.301\n",
      "INFO:tensorflow:step = 27401, loss = 0.000168051 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 445.242\n",
      "INFO:tensorflow:step = 27501, loss = 0.00143462 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 428.043\n",
      "INFO:tensorflow:step = 27601, loss = 0.000935573 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.263\n",
      "INFO:tensorflow:step = 27701, loss = 0.000289417 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.301\n",
      "INFO:tensorflow:step = 27801, loss = 0.000623281 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 455.408\n",
      "INFO:tensorflow:step = 27901, loss = 0.000400492 (0.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.52\n",
      "INFO:tensorflow:step = 28001, loss = 0.00141883 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.431\n",
      "INFO:tensorflow:step = 28101, loss = 0.000513484 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 429.887\n",
      "INFO:tensorflow:step = 28201, loss = 0.00133993 (0.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.125\n",
      "INFO:tensorflow:step = 28301, loss = 0.000679669 (0.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 403.781\n",
      "INFO:tensorflow:step = 28401, loss = 0.000811346 (0.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 405.425\n",
      "INFO:tensorflow:step = 28501, loss = 0.00057602 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 407.078\n",
      "INFO:tensorflow:step = 28601, loss = 0.000366546 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.43\n",
      "INFO:tensorflow:step = 28701, loss = 0.00086157 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 449.254\n",
      "INFO:tensorflow:step = 28801, loss = 0.00103206 (0.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.429\n",
      "INFO:tensorflow:step = 28901, loss = 0.000274243 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 433.628\n",
      "INFO:tensorflow:step = 29001, loss = 0.00182547 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.264\n",
      "INFO:tensorflow:step = 29101, loss = 0.00104729 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.261\n",
      "INFO:tensorflow:step = 29201, loss = 0.001766 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.521\n",
      "INFO:tensorflow:step = 29301, loss = 0.00105772 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 394.206\n",
      "INFO:tensorflow:step = 29401, loss = 0.000961037 (0.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 417.298\n",
      "INFO:tensorflow:step = 29501, loss = 0.00102937 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 415.558\n",
      "INFO:tensorflow:step = 29601, loss = 0.000430005 (0.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 410.429\n",
      "INFO:tensorflow:step = 29701, loss = 0.00025649 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 415.559\n",
      "INFO:tensorflow:step = 29801, loss = 0.000605404 (0.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 347.505\n",
      "INFO:tensorflow:step = 29901, loss = 0.00077305 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 365.326\n",
      "INFO:tensorflow:step = 30001, loss = 0.000108064 (0.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 386.568\n",
      "INFO:tensorflow:step = 30101, loss = 0.000590002 (0.258 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.301\n",
      "INFO:tensorflow:step = 30201, loss = 0.000234104 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.263\n",
      "INFO:tensorflow:step = 30301, loss = 0.00065963 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 445.241\n",
      "INFO:tensorflow:step = 30401, loss = 0.00115885 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 447.24\n",
      "INFO:tensorflow:step = 30501, loss = 0.000882715 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 426.213\n",
      "INFO:tensorflow:step = 30601, loss = 0.00145138 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.431\n",
      "INFO:tensorflow:step = 30701, loss = 0.000891694 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 447.239\n",
      "INFO:tensorflow:step = 30801, loss = 0.00118 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 429.887\n",
      "INFO:tensorflow:step = 30901, loss = 0.00115266 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.521\n",
      "INFO:tensorflow:step = 31001, loss = 0.000961674 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.263\n",
      "INFO:tensorflow:step = 31101, loss = 0.00149113 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.43\n",
      "INFO:tensorflow:step = 31201, loss = 0.000568497 (0.229 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 431.749\n",
      "INFO:tensorflow:step = 31301, loss = 0.000460989 (0.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 445.242\n",
      "INFO:tensorflow:step = 31401, loss = 0.000649023 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.303\n",
      "INFO:tensorflow:step = 31501, loss = 0.000197947 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.124\n",
      "INFO:tensorflow:step = 31601, loss = 6.59552e-05 (0.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 422.602\n",
      "INFO:tensorflow:step = 31701, loss = 0.000625275 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.301\n",
      "INFO:tensorflow:step = 31801, loss = 9.4202e-05 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.301\n",
      "INFO:tensorflow:step = 31901, loss = 0.000756386 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 417.298\n",
      "INFO:tensorflow:step = 32001, loss = 0.000197061 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 417.298\n",
      "INFO:tensorflow:step = 32101, loss = 0.000425281 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.357\n",
      "INFO:tensorflow:step = 32201, loss = 0.00102074 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 445.242\n",
      "INFO:tensorflow:step = 32301, loss = 0.00041044 (0.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 413.834\n",
      "INFO:tensorflow:step = 32401, loss = 0.000104019 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 403.781\n",
      "INFO:tensorflow:step = 32501, loss = 0.000511087 (0.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 407.079\n",
      "INFO:tensorflow:step = 32601, loss = 0.000111053 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 419.051\n",
      "INFO:tensorflow:step = 32701, loss = 0.00126878 (0.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 431.75\n",
      "INFO:tensorflow:step = 32801, loss = 0.000895224 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 417.297\n",
      "INFO:tensorflow:step = 32901, loss = 0.000406547 (0.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.401\n",
      "INFO:tensorflow:step = 33001, loss = 0.000497633 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 413.834\n",
      "INFO:tensorflow:step = 33101, loss = 0.00210473 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 405.424\n",
      "INFO:tensorflow:step = 33201, loss = 0.000945465 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 420.82\n",
      "INFO:tensorflow:step = 33301, loss = 0.000309007 (0.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 428.043\n",
      "INFO:tensorflow:step = 33401, loss = 0.00123292 (0.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 422.602\n",
      "INFO:tensorflow:step = 33501, loss = 0.000225348 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.124\n",
      "INFO:tensorflow:step = 33601, loss = 0.000940695 (0.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 389.588\n",
      "INFO:tensorflow:step = 33701, loss = 0.000726792 (0.257 sec)\n",
      "INFO:tensorflow:global_step/sec: 407.077\n",
      "INFO:tensorflow:step = 33801, loss = 0.00123973 (0.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 369.386\n",
      "INFO:tensorflow:step = 33901, loss = 0.00107691 (0.271 sec)\n",
      "INFO:tensorflow:global_step/sec: 407.076\n",
      "INFO:tensorflow:step = 34001, loss = 0.000640319 (0.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 408.747\n",
      "INFO:tensorflow:step = 34101, loss = 0.000919945 (0.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 388.072\n",
      "INFO:tensorflow:step = 34201, loss = 0.00121334 (0.258 sec)\n",
      "INFO:tensorflow:global_step/sec: 388.071\n",
      "INFO:tensorflow:step = 34301, loss = 0.000127433 (0.258 sec)\n",
      "INFO:tensorflow:global_step/sec: 394.205\n",
      "INFO:tensorflow:step = 34401, loss = 0.000417725 (0.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 426.216\n",
      "INFO:tensorflow:step = 34501, loss = 0.000630038 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 400.538\n",
      "INFO:tensorflow:step = 34601, loss = 0.000360699 (0.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 422.602\n",
      "INFO:tensorflow:step = 34701, loss = 0.00173927 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 417.298\n",
      "INFO:tensorflow:step = 34801, loss = 0.000371267 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 356.193\n",
      "INFO:tensorflow:step = 34901, loss = 0.000374646 (0.281 sec)\n",
      "INFO:tensorflow:global_step/sec: 413.835\n",
      "INFO:tensorflow:step = 35001, loss = 0.000305932 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 377.78\n",
      "INFO:tensorflow:step = 35101, loss = 0.00104693 (0.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 382.124\n",
      "INFO:tensorflow:step = 35201, loss = 0.000340343 (0.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 407.078\n",
      "INFO:tensorflow:step = 35301, loss = 0.000153641 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 325.928\n",
      "INFO:tensorflow:step = 35401, loss = 0.000996451 (0.306 sec)\n",
      "INFO:tensorflow:global_step/sec: 326.998\n",
      "INFO:tensorflow:step = 35501, loss = 0.000171532 (0.306 sec)\n",
      "INFO:tensorflow:global_step/sec: 392.792\n",
      "INFO:tensorflow:step = 35601, loss = 0.000374124 (0.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 405.423\n",
      "INFO:tensorflow:step = 35701, loss = 0.000663798 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.262\n",
      "INFO:tensorflow:step = 35801, loss = 0.000831119 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 429.889\n",
      "INFO:tensorflow:step = 35901, loss = 0.000574592 (0.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 426.214\n",
      "INFO:tensorflow:step = 36001, loss = 0.000313818 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 431.75\n",
      "INFO:tensorflow:step = 36101, loss = 0.00114753 (0.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 374.939\n",
      "INFO:tensorflow:step = 36201, loss = 0.000859921 (0.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 400.54\n",
      "INFO:tensorflow:step = 36301, loss = 9.09132e-05 (0.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 405.422\n",
      "INFO:tensorflow:step = 36401, loss = 0.000575347 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 405.425\n",
      "INFO:tensorflow:step = 36501, loss = 0.000190056 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 431.749\n",
      "INFO:tensorflow:step = 36601, loss = 0.000239809 (0.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 403.782\n",
      "INFO:tensorflow:step = 36701, loss = 0.00044674 (0.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 391.114\n",
      "INFO:tensorflow:step = 36801, loss = 0.000904511 (0.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 315.615\n",
      "INFO:tensorflow:step = 36901, loss = 0.000720104 (0.319 sec)\n",
      "INFO:tensorflow:global_step/sec: 369.385\n",
      "INFO:tensorflow:step = 37001, loss = 0.000412932 (0.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 360.052\n",
      "INFO:tensorflow:step = 37101, loss = 0.000163695 (0.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 383.732\n",
      "INFO:tensorflow:step = 37201, loss = 0.000264861 (0.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 376.354\n",
      "INFO:tensorflow:step = 37301, loss = 0.000719457 (0.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 383.594\n",
      "INFO:tensorflow:step = 37401, loss = 0.000339975 (0.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 389.586\n",
      "INFO:tensorflow:step = 37501, loss = 0.000324403 (0.258 sec)\n",
      "INFO:tensorflow:global_step/sec: 377.78\n",
      "INFO:tensorflow:step = 37601, loss = 0.000306283 (0.264 sec)\n",
      "INFO:tensorflow:global_step/sec: 386.567\n",
      "INFO:tensorflow:step = 37701, loss = 0.000100169 (0.263 sec)\n",
      "INFO:tensorflow:global_step/sec: 394.206\n",
      "INFO:tensorflow:step = 37801, loss = 0.000432111 (0.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 402.155\n",
      "INFO:tensorflow:step = 37901, loss = 0.00140533 (0.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.125\n",
      "INFO:tensorflow:step = 38001, loss = 0.000294866 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.937\n",
      "INFO:tensorflow:step = 38101, loss = 0.00119604 (0.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 376.355\n",
      "INFO:tensorflow:step = 38201, loss = 0.000891358 (0.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 362.67\n",
      "INFO:tensorflow:step = 38301, loss = 0.000129545 (0.276 sec)\n",
      "INFO:tensorflow:global_step/sec: 370.759\n",
      "INFO:tensorflow:step = 38401, loss = 0.000193591 (0.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 370.759\n",
      "INFO:tensorflow:step = 38501, loss = 0.00049303 (0.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 366.67\n",
      "INFO:tensorflow:step = 38601, loss = 0.000675174 (0.272 sec)\n",
      "INFO:tensorflow:global_step/sec: 380.664\n",
      "INFO:tensorflow:step = 38701, loss = 0.000358051 (0.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 392.655\n",
      "INFO:tensorflow:step = 38801, loss = 0.000242674 (0.257 sec)\n",
      "INFO:tensorflow:global_step/sec: 374.941\n",
      "INFO:tensorflow:step = 38901, loss = 0.00127899 (0.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 374.94\n",
      "INFO:tensorflow:step = 39001, loss = 0.000262657 (0.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 389.587\n",
      "INFO:tensorflow:step = 39101, loss = 0.000705544 (0.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 388.071\n",
      "INFO:tensorflow:step = 39201, loss = 0.000659797 (0.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 389.585\n",
      "INFO:tensorflow:step = 39301, loss = 0.000316944 (0.257 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 376.356\n",
      "INFO:tensorflow:step = 39401, loss = 0.000639012 (0.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 386.567\n",
      "INFO:tensorflow:step = 39501, loss = 0.000241011 (0.257 sec)\n",
      "INFO:tensorflow:global_step/sec: 372.143\n",
      "INFO:tensorflow:step = 39601, loss = 0.000744708 (0.269 sec)\n",
      "INFO:tensorflow:global_step/sec: 372.141\n",
      "INFO:tensorflow:step = 39701, loss = 0.000178735 (0.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 389.587\n",
      "INFO:tensorflow:step = 39801, loss = 0.00115475 (0.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 408.747\n",
      "INFO:tensorflow:step = 39901, loss = 0.00126455 (0.246 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 40000 into C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\tmp9g7bmdju\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.000402969.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SKCompat()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.contrib.learn.RunConfig(tf_random_seed=42) # not shown in the config\n",
    "\n",
    "feature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)\n",
    "dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300,100], n_classes=10,\n",
    "                                         feature_columns=feature_cols, config=config)\n",
    "dnn_clf = tf.contrib.learn.SKCompat(dnn_clf) # if TensorFlow >= 1.1\n",
    "dnn_clf.fit(X_train, y_train, batch_size=50, steps=40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Constructuon phase\n",
    "- 텐서플로우 라이브러리 가져오기\n",
    "- 입력 및 출력 수 지정\n",
    "- 각 계층에서 hidden neurons의 수를 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#두 개의 히든레이어(하나는 300개의 뉴런, 다른 하나는 100개의 뉴런)\n",
    "#10개의 뉴런을 가지고 있는 sotfmax output layer(숫자 0~9)\n",
    "# MNIST 데이터 784개의 feature(28*28)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- placeholder 노드를 사용하여 학습데이터와 타겟을 나타낼 수 있다.\n",
    "- x의 shape은 부분적으로만 정의. None.\n",
    "-  우리는 첫 번째 차원의 인스턴스와, 두 번째 차원의 feature를 갖는 2차원 텐서(즉, 행렬)라는 것을 알 수 있다.\n",
    "- 또한, feature의 개수는 28*28(픽섹 당 하나의 feature)라는 것을 알 수 있다.\n",
    "- 하지만, 각 학습 batch에 몇 개의 인스턴스가 포함되는지 아직 알 수 없다. 그렇기 때문에 X의 shape는 (None, n_inputs)이 된다.\n",
    "- 마찬가지로 y는 인스턴스 당 하나의 엔트리를 갖는 1차원 텐서가 될 것이지만 이 시점에서 학습 batch의 크기를 알 수 없기 때문에 shape는 (None)이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **이제 실제 신경망을 만들어보자.**\n",
    "- placeholder X는 input layer로 작동한다. 이는 execution phase에서 한 번에 하나의 학습 batch로 대체된다. (학습 batch의 모든 인스턴스는 신경망에 의해 동시에 처리된다)\n",
    "- 그리고 두 개의 hidden layer와 output layer를 만들어야된다. 두 개의 hidden layer는 거의 동일하다. 차이점은 단지 연결되는 input과 그들을 포함하는 뉴런의 개수 정도이다. \n",
    "- output layer도 매우 유사하지만 ReLU 활성화 함수 대신 softmax 활성화 함수를 사용한다. 한 번에 하나의 layer를 만드는 데 사용할 neuron_layer() 함수를 만든다. 이는 입력, 뉴런 수, 활성화 함수 및 layer 이름을 지정하는 파라미터를 필요로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** 코드 상세 **\n",
    "\n",
    "  - 1)with tf.name_scope(name): layer 이름을 사용하여 name scope을 만든다. 이 레이어에 대한 모든 계산 노드가 포함된다.\n",
    "  - 2)n_inputs = int(X.ger_shape()[1]): 입력 행렬(input matrix)의 shape에서 두 번째 차원의 크기를 불러와서 input의 수를 얻는다(첫 번째 차원은 인스턴스용)\n",
    "  - 3)stddev, init, W: 가중치 행렬(weight matrix)(layer's kernel 이라고도 부름)을 hold할 변수 W를 만든다. 각 입력과 각 뉴런사이의 모든 연결 가중치를 포함하는 2차원 텐서이다. 따라서 그 모양은(n_inputs, n_neurons)이 된다. \n",
    "  표준편차가 <img src='10-10.png'> 인 truncate된 정규분포를 사용하여 무작위로 초기화 한다. 이 측정 표준편차를 사용하면 알고리즘이 훨씬 빨리 수렴하는데 도움이 된다.(11장에서 상세히 다룰 예정). Gradient Descent의 대칭을 피하기 위해 모든 히든레이어에 대해 연결 가중치를 임의로 초기화하는것이 중요함.\n",
    "  - 4)b = tf.Variable(tf.zeros([n_neurons]), name='bias'): 0으로 초기화 된 bias에 대한 변수b 를 만든다. 뉴런 당 하나의 bias 파라미터가 있다.\n",
    "  - 5)Z = tf.matmul(X, W) + b: 그런 다음 Z = X · W + b를 계산하기 위한 부분 그래프(subgraph)를 만든다. 이 벡터화 된 구현은 한번에 batch 내 모든 인스턴스에 대해 입력의 가중치 합계와 (weighted sums of the inputs) layer 내 각 뉴런에 대한 bias term을 효율적으로 계산한다.\n",
    "  - 6)if activation is not None: ~ return Z: 마지막으로 tf.nn.relu (즉, max(0,Z))와 같은 활성화 파라미터가 제공되면, 코드는 활성화(activation) (Z)를 반환하거나 그렇지 않으면 Z만 반환한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제는 neuron layer를 만들기 위해 멋진 함수를 생성했다(neuron_layer). \n",
    "- 이 함수는 deep neural network를 만들기 위해 사용된다! 첫 번째 hidden layer는 X를 입력으로 사용한다. - - 두 번째 hidden layer는 첫 번째 hidden layer의 출력을 입력으로 사용한다. \n",
    "- 마지막으로, output layer는 두 번째 hidden layer의 출력을 입력으로 사용한다.(X -> first hidden layer -> second hidden layer -> output layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\",\n",
    "                           activation=tf.nn.relu)\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                           activation=tf.nn.relu)\n",
    "    logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다시 한번 우리는 명확성을 위해 name scope를 사용했다. \n",
    "- 또한 logits은 softmax 활성화 함수를 거치기 전에 neural network의 output이다. 최적화를 위해 나중에softmax 계산을 처리할 것이다.\n",
    "-  텐서플로우의 tf.layers.dense() 함수는 모든 input이 layer의 모든 neuron에 연결되는 fully connected layer를 만든다.(neuron_layer() 함수 대신 dense() 함수를 사용하도록 조정해보자.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 신경망 모델을 사용할 준비가 되었으므로, 이를 학습시키는 데 사용할 **비용함수를 정의해야 한다.**\n",
    "- softmax regression과 마찬가지로 교차 엔트로피를 사용한다.\n",
    "- 교차 엔트로피는 목표 클래스에 대한 낮은 확률을 추정하는 모델에 패널티를 준다.\n",
    "- **텐서플로에는 교차 엔트로피를 계산하는 몇 가지 함수를 제공한다.**\n",
    "  - sparse_softmax_cross_entropy_with_logits()은 \"logits\" (즉, softmax 활성화 함수를 거치기 전 네트워크의 output)을 기준으로 교차 엔트로피를 계산한다. 그리고 0에서 클래스의 수에서 1을 뺀 범위의 정수 형태인 label을 예상한다. (이 경우,0~9) \n",
    "  - 이것은 각 인스턴스에 대해 교차 엔트로피를 포함하는 1차원 텐서를 제공한다. 그런 다음 텐서플로우의reduce_mean() 함수를 이용하여 모든 인스턴스에 대한 평균 교차 엔트로피를 계산할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 신경망 모델을 가지고 있고, 비용 함수 또한 가지고 있다. \n",
    "- 그렇다면, 비용함수를 최소화하기 위해 모델의 파라미터를 조정하기 위한 GradientDescentOptimizer를 정의해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- construction phase의 마지막 중요한 단계는 모델을 평가하는 방법을 지정하는 것이다. 단순히 성능 척도로서 accuracy를 이용할 것이다. \n",
    "  - 첫째, 각 인스턴스에 대해 가장 높은 logit이 target 클래스에 해당되는지에 대한 여부를 확인하여 신경망의 예측이 올바른지 확인한다. \n",
    "  - 이를 위해 in_top_k() 함수를 사용할 수 있다. boolean(0또는 1로 참거짓을 구분) 값의 1차원 텐서를 반환하기 때문에 이 boolean 값을 float형으로 캐스팅 한 다음 평균을 계산해야한다. 이렇게 하면 네트워크의 전체 accuracy를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그 다음으로 평소처럼, 모든 변수를 초기화하는 노드를 만들어야하며, 학습 된 모델 파라미터 변수를 디스크에 저장하는 Saver도 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **이것으로 constriction phase가 끝났다. 이것은 40줄 미만의 코드였지만,** \n",
    "\n",
    "  - 1) input과 target에 대한 placeholder를 만들었고, \n",
    "  - 2) neuron layer를 만드는 함수를 만들고, \n",
    "  - 3) DNN을 만들기 위해 비용함수(cost function)를 정의했으며, \n",
    "  - 4) 최적화 도구(optimizer)를 만들었고, \n",
    "  - 5) 마지막으로 성능 측정(Performance measure)을 정의했다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Execution phase\n",
    "- 이 부분은 구축단계보다 훨씬 더 짧고 간단하다.\n",
    "- 먼저 MNIST 로드, 텐서플로우는 데이터를 가져와서(fetch) 0에서 1사이로 크기를 조정하고(scale) 셔플링하며 하나의 mini-batch를 로드하는 간단한 함수를 제공하는 자체 helper를 제공.\n",
    "\n",
    "\n",
    "- 실행하려는 epoch의 수와, mini-batch의 크기를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 40 #데이터를 몇 번 학습 시킬지\n",
    "batch_size = 50 #입력데이터 갯수? 학습 셋 크기?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **모델 학습 시작**\n",
    "-  아래 코드는 텐서플로우 세션을 열고 모든 변수를 초기화하는 init 노드를 실행한다. \n",
    "- 그런 다음 기본 학습 루프를 실행한다. 각 epoch에서 학습 세트 크기에 해당하는 mini-batch의 수만큼 반복한다.\n",
    "- 각 mini-batch는 next_batch() 메소드를 통해 데이터를 가져온 후(fetch), 학습 작업(training operation)을 실행하고, 현재 mini-batch의 input 데이터와 target을 feeding 한다. \n",
    "- 그 다음으로, 각 epoch의 마지막에서 last mini-batch와 전체 테스트 세트의 모델을 평가하고, 결과를 출력한다. \n",
    "- 마지막으로 모델 파라미터 변수가 디스크에 저장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.9 Val accuracy: 0.9146\n",
      "1 Train accuracy: 0.94 Val accuracy: 0.9348\n",
      "2 Train accuracy: 0.92 Val accuracy: 0.9466\n",
      "3 Train accuracy: 0.96 Val accuracy: 0.9508\n",
      "4 Train accuracy: 0.92 Val accuracy: 0.9586\n",
      "5 Train accuracy: 0.94 Val accuracy: 0.9586\n",
      "6 Train accuracy: 0.98 Val accuracy: 0.961\n",
      "7 Train accuracy: 0.96 Val accuracy: 0.9636\n",
      "8 Train accuracy: 0.92 Val accuracy: 0.964\n",
      "9 Train accuracy: 0.96 Val accuracy: 0.965\n",
      "10 Train accuracy: 0.98 Val accuracy: 0.969\n",
      "11 Train accuracy: 0.94 Val accuracy: 0.9684\n",
      "12 Train accuracy: 1.0 Val accuracy: 0.9704\n",
      "13 Train accuracy: 0.94 Val accuracy: 0.9686\n",
      "14 Train accuracy: 1.0 Val accuracy: 0.9716\n",
      "15 Train accuracy: 1.0 Val accuracy: 0.9732\n",
      "16 Train accuracy: 1.0 Val accuracy: 0.9738\n",
      "17 Train accuracy: 0.98 Val accuracy: 0.9732\n",
      "18 Train accuracy: 1.0 Val accuracy: 0.9752\n",
      "19 Train accuracy: 1.0 Val accuracy: 0.9748\n",
      "20 Train accuracy: 0.98 Val accuracy: 0.9748\n",
      "21 Train accuracy: 1.0 Val accuracy: 0.975\n",
      "22 Train accuracy: 1.0 Val accuracy: 0.9756\n",
      "23 Train accuracy: 1.0 Val accuracy: 0.9772\n",
      "24 Train accuracy: 1.0 Val accuracy: 0.978\n",
      "25 Train accuracy: 1.0 Val accuracy: 0.9774\n",
      "26 Train accuracy: 1.0 Val accuracy: 0.9776\n",
      "27 Train accuracy: 1.0 Val accuracy: 0.9764\n",
      "28 Train accuracy: 0.98 Val accuracy: 0.9776\n",
      "29 Train accuracy: 1.0 Val accuracy: 0.9778\n",
      "30 Train accuracy: 1.0 Val accuracy: 0.977\n",
      "31 Train accuracy: 1.0 Val accuracy: 0.9786\n",
      "32 Train accuracy: 0.98 Val accuracy: 0.9774\n",
      "33 Train accuracy: 0.98 Val accuracy: 0.979\n",
      "34 Train accuracy: 0.98 Val accuracy: 0.9782\n",
      "35 Train accuracy: 0.98 Val accuracy: 0.9772\n",
      "36 Train accuracy: 0.98 Val accuracy: 0.9794\n",
      "37 Train accuracy: 1.0 Val accuracy: 0.9786\n",
      "38 Train accuracy: 1.0 Val accuracy: 0.9792\n",
      "39 Train accuracy: 1.0 Val accuracy: 0.9788\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,\n",
    "                                            y: mnist.validation.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val)\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 using the Neural Network\n",
    "- 신경망이 학습되었으므로, 이를 이용하여 예측할 수 있다.\n",
    "- 동일한 construction phase를 재사용 할 수 있지만, 다음과 같이 execution phase를 변경 해 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./my_model_final.ckpt\n\t [[Node: save/RestoreV2_3 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_3/tensor_names, save/RestoreV2_3/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_3', defined at:\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python35\\Lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python35\\Lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-74f077792af3>\", line 2, in <module>\n    saver = tf.train.Saver()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1139, in __init__\n    self.build()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1170, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 640, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./my_model_final.ckpt\n\t [[Node: save/RestoreV2_3 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_3/tensor_names, save/RestoreV2_3/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\Lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./my_model_final.ckpt\n\t [[Node: save/RestoreV2_3 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_3/tensor_names, save/RestoreV2_3/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-5963f09b8d74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"./my_model_final.ckpt\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#디스크에서 모델 파라미터 변수를 로드\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mX_new_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#분류하기 위한 새로운 이미지 로드\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_new_scaled\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#학습 데이터와 동일한 feature scaling을 해야한다.(0에서 1로 스케일 조정)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#그런다음, logits 노드를 평가 한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1546\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Restoring parameters from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1547\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1548\u001b[1;33m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./my_model_final.ckpt\n\t [[Node: save/RestoreV2_3 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_3/tensor_names, save/RestoreV2_3/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_3', defined at:\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python35\\Lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python35\\Lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-74f077792af3>\", line 2, in <module>\n    saver = tf.train.Saver()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1139, in __init__\n    self.build()\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1170, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 640, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"c:\\users\\hp\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./my_model_final.ckpt\n\t [[Node: save/RestoreV2_3 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_3/tensor_names, save/RestoreV2_3/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\") #디스크에서 모델 파라미터 변수를 로드\n",
    "    X_new_scaled = mnist.test.images[:20] #분류하기 위한 새로운 이미지 로드\n",
    "    Z = logits.eval(feed_dict={X: X_new_scaled})#학습 데이터와 동일한 feature scaling을 해야한다.(0에서 1로 스케일 조정)\n",
    "    #그런다음, logits 노드를 평가 한다.\n",
    "    y_pred = np.argmax(Z, axis=1)\n",
    "    #모든 예측된 클래스의 확률을 알고 싶다면 softmax()함수를 logits에 적용해야되지만, \n",
    "    #단지 클래스를 예측하기를 원한다면 argmax() 함수를 이용하면 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-9be920125bb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Predicted classes:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Actual classes:   \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Predicted classes:\", y_pred)\n",
    "print(\"Actual classes:   \", mnist.test.labels[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
