{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter 11. Training Deep neural nets\n",
    "\n",
    "- 고해상도의 이미지에서 수백가지 유형의 객체를 감지해야 하는 것 처럼 복잡한 문제는 어떻게 해결할까?\n",
    "- 훨씬 어려운 DNN 학습이 필요할 수 있으며 이러한 작업은 쉬운일이 아닌다. 몇가지 이유는 다음과 같다.\n",
    "  - 기울기가 사라지는 (vanishing gradients) 문제 혹은 발산해버리는 (exploding gradients) 문제에 직면하게 되어 하위 레이어에서 학습하는 것을 매우 어렵게 만듦\n",
    "  - 대규모 네트워크에서 학습은 매우 느림\n",
    "  - 수백만 개의 파라미터가 있는 모델은 학습 세트를 오버피팅 할 위험이 있음\n",
    "- 이번 장에서는 이러한 문제들을 각각 살펴보고 해결할 기술을 제시한다.\n",
    "  - 기울기가 사라지는 문제 설명, 대중적으로 사용하는 해결책을 모색\n",
    "  - 다양한 기울기 하강법을 비교하여 대용량 모델을 빠르게 학습할 수 있는 다양한 최적화 기법에 대해 살펴 봄\n",
    "  - 대규모 신경망에 대한 몇 가지 인기있는 정규화 기법에 대해 살펴 봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vanishing & Exploding Gradients Problems\n",
    "- 역전파(backpropagation)알고리즘은 아웃레이어에서 인풋레이어로 이동하면서 Error gradient를 전파한다.\n",
    "- 알고리즘은 네트워크의 각 파라미터와 관련하여 비용함수의 기울기를 계산하여 각 파라미터를 gradient descent 단계로 갱신한다\n",
    "- 알고리즘이 하위 레이어로 진행함에 다라 기울기는 점점 작아진다. 즉, 레이어를 지날 때마다 최초 값보다 현저하게 작아지기 때문에 값을 전달해도 의미가 없다. -> 좋은 솔루션 아님. ---> **\"Vanishing gradient 문제\"**\n",
    "- 기울기가 커지는 반대의 경우도 발생하기 때문에 알고리즘은 발산하게 되는데 이를 ---> **\"Exploding gradients 문제\"** (14장 RNN에서 주로 발생하는 문제)\n",
    "\n",
    "- 2010년,  Xavier Glorot와 Yoshua Bengio가 작성한 \"Understanding the Difficulty of Traning Deep FeedForward Neural etworks\"라는 제목의 논문에서 몇 가지 의문점(suspects)을 발견하였다. \n",
    "  - 당시 인기있었던 logistic sigmoid 활성화 함수와 평균 0과 표준편차 1의 정규분포를 사용하는 무작위 초기화로 사용된 가중치 초기화(weight initialization)를 포함.\n",
    "  - 시그모이드 활성화 함수와 initialization을 통해 각 레이어의 아웃풋의 분산이 인풋의 분산보다 훨씬 크다는 것을 보여주었다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEOCAYAAACO+Hw9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyTj4zcyScfCbmSXj4DczS8bBb2aWzBvwhE2XtebJwAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b9467f1d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0,0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- logistic 활성화 함수를 보면 input 값이 커지면(음수 또는 양수)함수가 0 또는 1로 포화되고 도함수(미분계수)는 0에 가까워짐을 알 수 있다.\n",
    "- 따라서, 역전파가 시작 될 때, 네트워크를 통해 다시 전파 될 수 있는 gradient가 거의 없다. 역전파가 최상위 레이어를 통해 진행됨에 따라 gradient는 희박해지기 때문에 하위 레이어에는 아무 것도 남지 않는다.    ---(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Xavier and He Initialization\n",
    "\n",
    "- Glorot과 Bengio는 이 문제를 완화시키는 방법을 제안한다.\n",
    "- 신호가 양방향. 즉, 예측할 때 앞 방향으로, gradient를 역전파 할 때 반대 방향으로 적절히 흐르는 것을 필요로 한다.\n",
    "- layer가 동일한 input의 수와, output connection을 가지고 있지 않으면 보장 할 수는 없지만, 실제로 잘 잘동하는 것으로 입증 된 좋은 절충안을 제안했다.\n",
    "  - n_inputs과 n_outputs은 가중치가 초기화되는 input 및 output의 connections 수이다.(fan-in과 fan-out 이라고도 함)\n",
    "  - 이 초기화 전략은 Xavier initialization 또는 Glorot initialization 이라고도 한다.\n",
    "  - input 과 output의 연결 수가 대략 같으면 보다 간단한 방정식을 얻을 수 있다.\n",
    "  <img src='11-1.png'>\n",
    "  \n",
    "  - Xavier 초기화 전략을 사용하면 학습 속도가 상당히 빨라 질 수 있고 이는 딥러닝의 성공을 이끌어낸 트릭 중 하나이다. \n",
    "  - ReLU 활성화 함수에 대한 초기화 전략은 He initialization 이라고도 부른다.\n",
    "  \n",
    "  \n",
    "- 10장의 tf,layers.dense()함수는 균일 분포(uiform distribution)인 Xavier initialization을 사용. 아래와 같이 variance_scaling_initialization()함수를 사용하여 He initialization으로 변경 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()    #해당부분\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                          kernel_initializer=he_init, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.2 Nonsaturating Activation Functions\n",
    "\n",
    "-  Glorot과 Bengio의 2010년 논문으로부터 vanishing/exploding gradients 문제가 부분적으로 활성화 함수를 잘못 선택 되었기 때문에 발생한 것이라는것을 알 수 있었다. \n",
    "- 이 때까지만 해도, 대부분의 사람들은 생물학적 뉴런에서 sigmoid 활성화 함수를 사용하기로 결정하면, 훌륭한 선택일것이라고 생각했다.\n",
    "- 그러나 Deep Neural Network(DNN) 에서 다른 활성화 함수, 특히 ReLU 활성화 함수는 특히 더 잘 작동한다는 사실이 드러났다. ReLU 활성화 함수는 양수(positive) 값에 대해 포화되지 않기 때문이다. (또한 계산 속도가 매우 빠름)\n",
    "\n",
    "\n",
    "- 하지만  ReLU 활성화 함수는 완벽하지는 않다. 이것은 \"Dying ReLUs'로 알려진 문제로 인한 것이다. : 학습하는 동안 몇몇 뉴런은 실제적으로 죽게되는데, 이는 0이외에 다른 값의 출력을 하지 않는 것을 의미한다.(ReLU 함수는 0보다 낮은 값에서 Gradient 값은 0으로만 출력됨)\n",
    "- 경우에 따라서, 네트워크 학습 뉴런의 절만이 죽어있을 수도 있다.(특히, 학습률이 클수록)\n",
    "- 학습도중 뉴런의 입력 가중치의 합이 음수가 되도록, 뉴런의 가중치가 업데이트 된다면 이는 0을 출력하게 될 것이다. \n",
    "- 이 문제를 해결하기 위해 leaky ReLU와 같은 변형된 ReLU함수를 사용할 수 있다.\n",
    "\n",
    "<img src='11-2.png'>\n",
    "\n",
    "\n",
    "- 위와 같이 정의 할 수 있다.\n",
    "  - 파라미터 알파는 함수가 얼마나 많이 leaks 하는 지에 대해 정의한다.\n",
    "  - z<0 에 대한 함수의 gradient이며, 일반적으로 0.01로 설정된다.\n",
    " \n",
    " \n",
    "- 논문에서는 ReLU 활성화 함수의 여러 변종에 대해 비교를 했고, 결론 중 하나는 leaky ReLU 가 strict ReLU보다 우월하다는 것이다. 실제로 α=0.01(small leak)보다 성능이 좋게 나왔다.\n",
    "- 또한 randomized leaky ReLU(RReLU)에 대해서도 평가를 하였다. 여기에서 α는 학습 중 주어진 범위에서 무작위로 추출되며 테스트를 할 때에는 평균 값으로 고정된다. RReLU는 상당히 잘 수행되었고, regularizer 로써의 역할을 수행한다. 즉, 트레이닝 세트에서 과적합 되는 위함을 줄일 수 있었다.\n",
    "- 마지막으로 parametric leaky ReLU(PReLU)를 평가하였다. PReLU에서 α는 역전파에 의해 수정 될 수 있는 파라미터 값으로 학습 하는 동안 배울 수 있다. PReLU는 큰 이미지 데이터 세트에서 ReLU보다 강력하고 뛰어난 것으로 보고되었지만, 작은 이미지 데이터 세트에서는 학습셋이 과적합 될 수 있는 위험이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 텐서플로우에는 leaky ReLU에 대해 미리 정의된 함수는 없지만 정의하기 매우 쉽다.\n",
    "- alpha 값으로는 0.01로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):     #alpha\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEOCAYAAACkSI2SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAfHK9yS70NfMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b946ba3b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\") #activation=leaky_relu 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.86 Validation accuracy: 0.9044\n",
      "5 Batch accuracy: 0.94 Validation accuracy: 0.951\n",
      "10 Batch accuracy: 0.96 Validation accuracy: 0.9666\n",
      "15 Batch accuracy: 1.0 Validation accuracy: 0.972\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9748\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9764\n",
      "30 Batch accuracy: 0.98 Validation accuracy: 0.978\n",
      "35 Batch accuracy: 0.96 Validation accuracy: 0.9792\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_test = accuracy.eval(feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n",
    "            print(epoch, \"Batch accuracy:\", acc_train, \"Validation accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### ELU\n",
    "\n",
    "- 2016년 Djork-Arne Clevert의 논문에서 모든 ReLU의 변형을 능가하는 새로운 활성화 함수 Exponential linear unit(ELU)를 제안했다.\n",
    "- ELU는 학습 시간을 단축하고, 신경망이 테스트 세트에서 더 잘 수행됨을 확인하였다. ELU의 식과 그래프는 다음과 같다.\n",
    "\n",
    "<img src='11-3.png'>\n",
    "\n",
    "\n",
    "- ELU는 ReLU의 기능과 비교하면 몇 가지 큰 차이점을 제외하고는 비슷하다.\n",
    "  - (1) Z<0 일 때, 음의 값을 취한다. 단위가 0에 가까운 평균 출력을 갖는다. 이는 이전에 논의된 것처럼 vanishing gradient(기울기가 사라지는) 문제를 완화하는데 도움이 된다. 파라미터  α는 Z가 큰 음수일 때 ELU 함수가 접근하는 값을 정의한다. 일반적으로 1로 설정되지만, 다른 파라미터처럼 조정 가능하다.\n",
    "  - (2) Z<0에 대해 0이 아닌 기울기를 가지기 때문에, dying units 문제를 피할 수 있다.\n",
    "  - (3) 함수는 Z=0 주변을 포함하여 모든 면에서 smooth 하다. 그렇기 때문에 gradient descent 의 속도를 높이는데 도움이 된다.\n",
    "  \n",
    "  \n",
    "- ELU 활성화 함수에도 단점은 있다. 지수 함수의 사용으로 인하여 ReLU와 그 변형에 비해 느리게 계산된다. 학습동안에는 더 빠른 convergence rate로 보상을 받기는 하지만, 테스트 시간에 ELU는 ReLU보다 느리다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Tip]**\n",
    "\n",
    "활성화 함수가 상당히 많은데, 그렇다면 deep neural networks의 hidden layer에 어떤 활성화 함수를 사용해야할까?\n",
    " \n",
    "일반적으로 ELU > leaky ReLU(또는 그 변형) > ReLU > tanh > sigmoid 을 사용한다고 한다.\n",
    "만약 런타임 성능이 중요하다면 ELU 보다 leaky ReLU를 선호할 것이다.\n",
    "만약 또 다른 파라미터를 조정하고 싶지 않다면 이전에 제안한 기본값 α를 사용할 수 있다.(leaky ReLU의 경우 0.01, ELU의 경우 1)\n",
    "여유 시간과 컴퓨팅 파워가 있다면, cross validation을 이용하여 다른 활성화 함수, 특히 네트워크가 과적합하다면, RReLU를, 학습 세트가 너무 많을 경우에는 PReLU를 이용해 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEQCAYAAACtGP9YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e8934a9208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텐서플로우에는 신경망을 구축하는 데 사용할 수 있는 elu()함수를 제공.\n",
    "- 위 코드와 같이 dense()함수를 호출 할 때, activation 인수를 설정하면 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-  2015년 Segey Ioffe와 Christian Szegedy의 논문에서 vanishing/exploding 문제를 처리하기 위해 Batch Normalization(BN)이라는 기술을 제안 \n",
    "- 학습하는 동안 이전 레이어의 파라미터가 변경됨으로써, 각 레이어들의 입력 분포가 변경되는 문제 해결\n",
    "  - 각 레이어의 activation function 직전 모델에 operation을 추가, 간단히 input값들을 zero-centering과 normalizing을 한다.\n",
    "  - 그 다음, 레이어당 두 개의 새로운 파라미터를 사용하여 결과를 scaling 하고 shifting 하는 것으로 구성\n",
    "  - 즉, 하나의 파라미터는 scaling 용이며 다른 하나는 shifting 용이다.\n",
    "  - mini-batch를 통해 평균 및 표준편차를 평가하는 식은 아래와 같다.\n",
    "  \n",
    "  \n",
    "  <img src='11-4.png'>\n",
    "  \n",
    "  \n",
    "  - 테스트 할 때에는, 경험적 평균과 표준편차를 계산 할 mini-batch가 없으므로, 대신 전체 학습 세트의 평균 및 표준편차를 사용해야 한다. \n",
    "  - 이것들은 일반적으로 이동 평균(moving average)를 이용하여 학습 중에 효율적으로 계산된다. 따라서 전체적으로 각 batch-normalized layer에 4개의 파라미터가 학습된다. \n",
    "  - -> γ (scale), β (offset), μ(mean) 및 σ(standard deviation)\n",
    "  \n",
    "  \n",
    "- BN에도 단점은 존재한다. \n",
    "  - 비록 batch-normalized 상태일 때에는 첫 번째 hidden layer가 이를 처리하기 때문에 input 데이터를 정규화 할 필요는 제거 되지만, 모델에 약간의 복잡성이 추가된다.\n",
    "  - 또한 런타임 패널티가 있다. 신경망은 각 layer에서 필요한 추가 계산으로 인해 예측이 느려진다.\n",
    "  - 따라서 만약 예측을 빠르게 해야한다면, BN을 수행하기 전에 ELU와 He initialization이 제대로 수행되는지 확인이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1  Implementing Batch Normalization with TensorFlow\n",
    "\n",
    "-  텐서플로우는 input을 중앙에 놓고, normalize하는 tf.nn.batch_normalization() 함수를 제공하지만, 사용자가 직접 평균 및 표준편차를 계산해야 한다. (학습 중 mini-batch 데이터 또는 테스트중에 전체 데이터 셋을 기반) 그리고 파라미터를 함수에 전달하고 scaling 및 offset 파라미터의 생성을 처리해야 한다. (그리고 다시 함수에 전달) \n",
    "- 이것은 편리한 방법이 아니다. 대신 다음 코드와 같이 이 모든 것을 처리하는 tf.layers.batch_normalization() 함수를 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "                              training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.8697\n",
      "1 Test accuracy: 0.8988\n",
      "2 Test accuracy: 0.9128\n",
      "3 Test accuracy: 0.9208\n",
      "4 Test accuracy: 0.9284\n",
      "5 Test accuracy: 0.9366\n",
      "6 Test accuracy: 0.9395\n",
      "7 Test accuracy: 0.9435\n",
      "8 Test accuracy: 0.9475\n",
      "9 Test accuracy: 0.9506\n",
      "10 Test accuracy: 0.9519\n",
      "11 Test accuracy: 0.9544\n",
      "12 Test accuracy: 0.9578\n",
      "13 Test accuracy: 0.9575\n",
      "14 Test accuracy: 0.9599\n",
      "15 Test accuracy: 0.9611\n",
      "16 Test accuracy: 0.9603\n",
      "17 Test accuracy: 0.9636\n",
      "18 Test accuracy: 0.9644\n",
      "19 Test accuracy: 0.9657\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/gamma:0']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/moving_mean:0',\n",
       " 'batch_normalization/moving_variance:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/moving_mean:0',\n",
       " 'batch_normalization_1/moving_variance:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/moving_mean:0',\n",
       " 'batch_normalization_2/moving_variance:0']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.308\n",
      "1 Test accuracy: 0.792\n",
      "2 Test accuracy: 0.8781\n",
      "3 Test accuracy: 0.902\n",
      "4 Test accuracy: 0.9136\n",
      "5 Test accuracy: 0.9197\n",
      "6 Test accuracy: 0.9255\n",
      "7 Test accuracy: 0.9309\n",
      "8 Test accuracy: 0.9345\n",
      "9 Test accuracy: 0.9411\n",
      "10 Test accuracy: 0.9426\n",
      "11 Test accuracy: 0.9461\n",
      "12 Test accuracy: 0.9474\n",
      "13 Test accuracy: 0.9504\n",
      "14 Test accuracy: 0.9522\n",
      "15 Test accuracy: 0.9533\n",
      "16 Test accuracy: 0.9543\n",
      "17 Test accuracy: 0.9556\n",
      "18 Test accuracy: 0.9578\n",
      "19 Test accuracy: 0.9592\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Faster Optimizers\n",
    "\n",
    "-  매우 큰 심층 신경망(Deep Neural Network)을 학습하는 것은 고통스럽게 느릴 수 있다. 지금까지 학습을 가속화하고 더 나은 솔루션에 도달하는 4가지 방법에 대해 알아봤었다. 4가지라 함은, \n",
    "1) Applying a good initialization strategy for the connection weights\n",
    "2) Using a good activation function\n",
    "3) Using Batch Normalization\n",
    "4) Reusing parts of a pretrained network 이다.\n",
    "\n",
    "-  또 다른 속도 향상 방법으로 regular Gradient Descent optimizer보다 빠른 optimizer를 선택하는 방법이 있다. 이번 섹션에서는 가장 인기있는 것들에 대해 제시하고자 한다.\n",
    "   \n",
    "   - Momentum optimization\n",
    "   - Nesterov Accelerated Gradient\n",
    "   - AdaGrad\n",
    "   - RMSProp\n",
    "   - Adam optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Momentum optimization\n",
    "\n",
    "- 1964년 Polyak이 제안하였음. 볼링공의 경사면에서 굴러떨어지는 것에서 아이디어 얻음.이전 기울기가 무엇인지에 대해 크게 영향을 받는다.\n",
    "- 각 반복에서 momentum 벡터 m(learning rate η으로 곱한)에서 local gradient를 뺀 다음 이 momentum벡터를 단순히 추가하여 가중치를 업데이트 한다. \n",
    "- 즉, 속도가 아닌 가속도를 이용한다. \n",
    "\n",
    "<img src='11-5.png'>\n",
    "\n",
    "- 어떤 종류의 마찰 메커니즘을 시뮬레이션하고 운동량이 너무 커지지 않도록 하기 위해, 알고리즘은 0(high friction)과 1(no friction) 사이에서 설정해야 하는 momentum이라는 새로운 파라미터 β를 도입한다.\n",
    "- 일반적인 momentum 값은 0.9다\n",
    "-  Gradient가 일정하게 유지되면 terminal velocity(즉, 가중치 업데이트의 최대크기)가 learning rate η에 1/(1-β)를 곱한 값(부호 무시)에 gradient를 곱한것과 동일한지에 대해 쉽게 확인할 수 있다.\n",
    "- 신층신견망의 경우 Batch normalization을 사용하지 않기 때문에 상위 레이어는 종종 매우 다른 스케일의 입력을 갖는다. 이 때 momentum optimization을 사용하면 많은 도움이 된다.\n",
    "\n",
    "\n",
    "- 한 가지 단점은, 튜닝 할 파라미터가 추가된다는 것이다. 그러나 momentum 값 0.9는 일반적으로 잘 잘독하며 거의 항상 gradient descent 보다 빠르다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Nesterov Accelerated Gradient\n",
    "\n",
    "- 1983년 Yurii Nesterov가 제안한 momentum optimization의 변형은 vanilla momentum optimization(? 3.1) 보다 항상 빠르다.\n",
    "- Nesterov Accelerated Gradient(NAG)는 비용함수의 기울기를 local position이 아닌, momentum의 방향으로 약간 앞당기는 아이디어\n",
    "- vanilla momentum optimization과의 유일한 차이점은 기울기 θ가 아닌 θ+βm 에서 측정된다는 것\n",
    "- 원래 위치에서 gradient를 사용하는 대신, 그 방향으로 조금 더 멀리 측정 한 gradient를 사용하는 것이 좀 더 정확하다. \n",
    "  (∇1은 시작 포인트 θ에서 측정된 비용함수의 기울기를 나타내며, ∇2는 θ+βm에 위치한 점의 기울기를 나타낸다).\n",
    "  \n",
    "  <img src='11-6.png'>\n",
    "  \n",
    "  - 부가설명 필요. 질문 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9, use_nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 AdaGrad\n",
    "\n",
    "- 본 알고리즘은 가장 가파른 차원을 따라 기울기 벡터를 축소함으로써 이를 달성한다.\n",
    "\n",
    "<img src='11-7.png'>\n",
    "\n",
    "-  첫 번째 단계는 Gradient의 제곱을 벡터 s에 축적한다. (⊗ 기호는 요소별 곱셈(element-wise multiplication을 나타냄) 이 vectorized form은 벡터 s 의 각 요소 si에 대해 s_i ← s_i + (∂J (θ) / ∂ θ_i)^2 를 계산하는 것과 같다. 즉, 각 s_i는 파라미터 θ_i에 대한 비용 함수의 편미분의 제곱을 누적한다. 만약 비용 함수가 i 번째 차원에서 가파른 경우, s_i는 반복 할 때마다, 더욱 커지고 커질 것이다.\n",
    "\n",
    "-  두 번째 단계는 Gradient Descent와 거의 동일하지만 큰 차이점이 있다. Gradient 벡터는 루트(s+ε)의 인수로 축소된다. (⊘ 기호는 요소 단위의 분할(element-wise division)을 나타내고 ε은 0으로 나누는 것을 피하기 한 smoothing term이며, 일반적으로 10의 -10승으로 설정됨)\n",
    "\n",
    "\n",
    "<img src='11-8.png'>\n",
    "\n",
    "-  resulting updates를 global optimum을 향해 보다 직접적으로 가리킬 수 있다. 또한 추가적인 이점으로 learning rate 파라미터 η의 튜닝이 훨씬 덜 필요하다는 것이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 RMSProp\n",
    "\n",
    "-  3.3의 AdaGrad가 너무 느려지고 global optimum으로 수렴하지 못하는 경우에도 RMSProp알고리즘은 가장 최근 반복의 gradient만 축적함으로써 이를 수정한다. \n",
    "\n",
    "<img src='11-9.png'>\n",
    "\n",
    "- 첫 번째 단계에서 지수적 감쇠(exponential decay)를 사용하여 수행\n",
    "- 감쇠율(decay rate) β는 일반적으로 0.9로 설정된다. 이것은 새로운 파라미터이다. 그러나 베타값 0.9는 일반적으로 잘 동작하기 때문에 조정할 필요가 전혀 없을 수도 있다. 텐서플로우에는 RMSPropOptimizer클래스가 있다.\n",
    "- 3.1 이나 3.2 보다 빠르게 수렵하고, 3.3 보다 잘 작동하여 많은 연구자들이 선호하는 최적화 알고리즘 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n",
    "                                      momentum=0.9, decay=0.9, epsilon=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Adam Optimization\n",
    "\n",
    "-  Adaptive moment estimation을 나타내는 Adam은 momentum optimization과 RMSProp의 아이디어를 결합하였다\n",
    "\n",
    "<img src='11-10.png'>\n",
    "\n",
    "- T는 반복 횟수를 나타낸다. 1부터 시작함.\n",
    "-  1,2,5 단계만 보면 Adam optimization이 Momentum optimization과 RMSProp과 유사하다는 것을 알 수 있다.\n",
    "- 유일한 차이점은 단계1에서 exponentially decaying sum이 아닌 exponential decaying average를 계산한다는 것이다.  그러나 이는 상수 요소(constant factor)을 제외하고는 실제로 동일하다.(decaying average는 단지 decaying sum의 (1-β_1)배다.)\n",
    "- 3단계와 4단계는 다소 기술적인 세부사항이다. m과 x는 0에서 초기화되기 때문에 학습 시작시 0에 편향되므로, 이 두 단계는 학습 시작 시 m과 x를 향상시키는 데 도움이 된다. \n",
    "-  Adam은 AdaGrad와 RMSProp과 같은 adaptive learning rate 알고리즘이기 때문에, learning rate 파라미터 η의 조정이 덜 필요하다. 기본값인 0.001을 사용하면 Adam을 Gradient Descent보다 더 쉽게 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Learning Rate Scheduling\n",
    "\n",
    "<img src='11-11.png'>\n",
    "\n",
    "-  좋은 Learning rate를 찾는 것은 다소 까다로울 수 있다. 너무 높게 설정하면 발산(diverge)할 수 있다. \n",
    "-  너무 낮게 설정하면 학습은 결국 optimum으로 수렴은 되지만, 매우 오랜 시간이 걸릴 것이다.  \n",
    "- 약간 높게 설정하면 처음에는 매우 빠르게 진행되지만, optimum 근처에서 매우 여기저기 움직이며 끝나지 않을 것이다. \n",
    "- 만약 높은 learning rate로 시작한 다음, 빠르게 진행하지 않을 때, learning rate를 줄이면 최적의 constant learning rate보다 빠른 속도로 좋은 솔루션에 도달 할 수 있다. \n",
    "- 학습 중 learning rate를 줄이기 위한 여러가지 전략이 있다. 이러한 전략을 learning schedules 이라고 부른다. \n",
    "  - (1)예정된 구분 상수 학습률: 학습 률을 처음에는 η0 = 0.1로 설정하고 50 에포크 이후에는 η1 = 0.001로 설정. 효과적일 수는 잇지만 적절한 학습 속도와 사용시기를 파악하기 위해 항상 주의를 기울여야 한다.\n",
    "  - (2)성능 스케줄링: N단계마다 유효성 검증 오류를 측정하고 오류가 중단되지 않을때 학습률을 λ만큼 줄인다.\n",
    "  - (3)지수적 스케줄링: 학습속도를 반복 수 t의 함수로 설정.t: η(t) = η_0 10^(– t/r). 훌륭하게 작동 하지만  η_0과 r을 튜닝해야한다. 학습속도는 매 단계마다 10배 떨어진다.\n",
    "  - (4)동력 스케줄링: 학습속도를 η(t) = η_0 (1 + t/r)^–c.로 설정한다. 하이퍼매개변수 c는 일반적으로 1로 설정된다. 이것은 지수 일정과 비슷하지만 학습속도가 훨씬 느려진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래코드 설명.\n",
    "  - 파라미터 값을 설정 한 후에는 현재 학습 반복 횟수를 추적하기 위해 nontrainable 변수 global_step(0으로 초기화) 을 생성한다. \n",
    "  - 그런 다음 텐서플로우의 exponential_decay() 함수를 사용하여 기하급수적으로 감소하는 learning_rate를 정의한다. (η0 = 0.1 및 r = 10,000)\n",
    "  - 다음으로, 이 감소하는 learning_rate를 사용하여 optimizer를 만든다.(이 예제에서는 momentum optimizer)\n",
    "  - 마지막으로 optimizer의 minimize() 메소드를 호출하여 학습 operation을 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):       # not shown in the book\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                               decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.9595\n",
      "1 Test accuracy: 0.9749\n",
      "2 Test accuracy: 0.9753\n",
      "3 Test accuracy: 0.9783\n",
      "4 Test accuracy: 0.9819\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
